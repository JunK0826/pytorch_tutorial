{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (summary)Part-02_DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab08-09 overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- more data\n",
    "- less features\n",
    "- early stopping\n",
    "- less networks\n",
    "- weight decay\n",
    "- drop out\n",
    "- batch normalization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "mu = x.mean(dim=0)\n",
    "sigma = x.std(dim=1)\n",
    "norm_x = (x-mu)/sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- sigmoid => gradient vanishing\n",
    "- linear => only single layer\n",
    "- ReLU => 0<x == 0 => 0<x gradient vanishing\n",
    "- Leaky_ReLU => 0<x == other gredient \n",
    "\"\"\"\n",
    "# make model class with ReLU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2,1)\n",
    "        self.linear2 = nn.Linear(1,1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1) # leakyrelu \n",
    "        self.relu = nn.ReLU() # relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. weight initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RBM: train weight each layer using autoencoder => too many time\n",
    "xavier initialization, he(kaiming) initailization\n",
    "\n",
    "nn.init.xavier_uniform_(nn.linear.weight)\n",
    "nn.init.xavier_normal_(nn.linear.weight)\n",
    "nn.init.kaiming_uniform_(nn.linear.weight)\n",
    "nn.init.kaiming_uniform_(nn.linear.weight)\n",
    "\"\"\"\n",
    "# make model class with weight initailization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # nn.init.xavier_uniform_(nn.linear.weight)\n",
    "        # nn.init.xavier_normal_(nn.linear.weight)\n",
    "        # nn.init.kaiming_uniform_(nn.linear.weight)\n",
    "        # nn.init.kaiming_uniform_(nn.linear.weight)\n",
    "        self.linear1 = nn.Linear(2,1)\n",
    "        nn.init.xavier_normal_(self.linear1.weight) # weight initialization\n",
    "        self.linear2 = nn.Linear(1,1)\n",
    "        nn.init.xavier_normal_(self.linear2.weight) # weight initialization\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model class with drop out\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear1 = nn.Linear(2,1)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(1,1)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "model = BinaryClassifier()\n",
    "nb_epochs = 100\n",
    "for e in range(nb_epochs):\n",
    "    model.train() # dropout\n",
    "    # ...\n",
    "\n",
    "with torch.no_grad(): # no gradient \n",
    "    model.eval() # no dropout\n",
    "    # ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batchnorm = nn.BatchNorm1d(32)\n",
    "        self.linear1 = nn.Linear(2,1)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(1,1)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.batchnorm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "model = BinaryClassifier()\n",
    "nb_epochs = 100\n",
    "for e in range(nb_epochs):\n",
    "    model.train() # batchnorm\n",
    "    # ...\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval() # no batchnorm\n",
    "    # ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
